{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88ed7a41-d76f-4485-bd2d-4585b67de3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.applications as apps\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8acee17-a6f9-4b36-99c3-ceb2495b8991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPU, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744116895.965145   34807 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def set_memory_growth():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "      except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "def set_memory_limit(memory_limit):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=memory_limit)]\n",
    "        )\n",
    "\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "#set_memory_limit(4096)\n",
    "set_memory_growth()\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7199ab32-2dd9-4954-89bc-f79724d31950",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 8\n",
    "PATIENCE = 5\n",
    "\n",
    "saved_models_dir = Path(\"../saved_models\")\n",
    "saved_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "histories_dir = Path(\"../histories\")\n",
    "histories_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd067d-0574-40b9-8056-6f005779c464",
   "metadata": {},
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603e5f2a-3699-43cb-8a37-230ab0540cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ruairi/.cache/kagglehub/datasets/bloox2/fieldplant/versions/1/train\n"
     ]
    }
   ],
   "source": [
    "# Download latest version of data\n",
    "image_dir = kagglehub.dataset_download(\"bloox2/fieldplant\")\n",
    "image_dir = Path(image_dir) / \"train\"\n",
    "print(\"Path to dataset files:\", image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62d7c5-a1aa-41ca-94bc-6a77bc736d81",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2498b5-beb6-4719-9187-aad8c0b1ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_preprocessing(app_name):\n",
    "    app = getattr(keras.applications, app_name)\n",
    "    model_name = dir(app)[0]\n",
    "    model = getattr(app, model_name)\n",
    "    input_shape = model().input_shape[1:]\n",
    "    model = model(include_top=False, input_shape=input_shape)\n",
    "    model.trainable = False\n",
    "    preprocessing = getattr(app, \"preprocess_input\")\n",
    "    return model, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4db47e-824e-4444-8229-20c5ab4214d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_classifier_layers(model_name):\n",
    "    if \"vgg\" in model_name:\n",
    "        return keras.Sequential([\n",
    "            keras.layers.GlobalAveragePooling2D(),\n",
    "            keras.layers.Dense(1024, activation=\"relu\"),\n",
    "            keras.layers.Dense(1024, activation=\"relu\") ])\n",
    "    else:\n",
    "        return keras.Sequential([keras.layers.GlobalAveragePooling2D()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41c77b0-169c-482a-85a8-cbf086a33f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(app_name, activation, num_classes):\n",
    "    model, preprocessing = get_model_and_preprocessing(app_name)\n",
    "    pre_classifier_layers = get_pre_classifier_layers(model.name)\n",
    "\n",
    "    inputs = keras.Input(shape=model.input_shape[1:])\n",
    "    x = preprocessing(inputs)\n",
    "    x = model(x, training=False)\n",
    "    x = pre_classifier_layers(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation=activation, name=\"classifier_layer\")(x)\n",
    "\n",
    "    model_name = model.name\n",
    "    model = keras.Model(inputs, outputs, name=model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db5128bc-fe08-46fe-8f41-6c5705e4d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(methodology):\n",
    "    multilabel = methodology == \"multilabel\"\n",
    "    \n",
    "    methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    losses = [\"categorical_crossentropy\", \"binary_crossentropy\"]\n",
    "    activation = [\"softmax\", \"sigmoid\"]\n",
    "    metrics = [\"categorical_accuracy\", \"binary_accuracy\"]\n",
    "\n",
    "    idx = methodologies.index(methodology)\n",
    "\n",
    "    metrics = [metrics[idx]]\n",
    "    f1_score_weighted = keras.metrics.F1Score(average=\"weighted\", threshold=0.5 if multilabel else None, name=\"f1_score_weighted\", dtype=None)\n",
    "    f1_score_per_class = keras.metrics.F1Score(average=None, threshold=0.5 if multilabel else None, name=\"f1_score_per_class\", dtype=None)\n",
    "    metrics.append(f1_score_weighted)\n",
    "    metrics.append(f1_score_per_class)\n",
    "        \n",
    "    hyperparams = [losses[idx], activation[idx], metrics]\n",
    "    \n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52871059-9ca3-435f-9cf0-d738f597d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model):\n",
    "    compile_config = model._compile_config.config\n",
    "    optimizer = compile_config['optimizer'].get_config()\n",
    "    classifier_activation = model.get_layer(name=\"classifier_layer\").activation.__name__\n",
    "\n",
    "    print(\"Model name:\", model.name)\n",
    "    print(\"Input shape:\", model.input_shape)\n",
    "    print(\"Optimizer name:\", optimizer['name'], \"learning_rate:\", np.round(optimizer['learning_rate'], 6))\n",
    "    print(\"Loss:\", compile_config['loss'])\n",
    "    print(\"Metrics:\")\n",
    "    for metric in compile_config['metrics']:\n",
    "        print(metric if isinstance(metric, str) else metric.get_config())\n",
    "    print(\"Classifier layer activation function:\", classifier_activation)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08bb57e7-5137-4267-9bd0-fc7dc629e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name, fine_tuning=False):\n",
    "    cbs = [\n",
    "        keras.callbacks.EarlyStopping(patience=PATIENCE, restore_best_weights=True, baseline=None, verbose=1),\n",
    "        keras.callbacks.ModelCheckpoint(filepath=f\"{saved_models_dir}/{model_name}.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1, initial_value_threshold=None)]\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d421be4-0a10-4ad9-8877-ed16835789b6",
   "metadata": {},
   "source": [
    "# GET DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bcc6399-8c40-434a-b4ff-082e09a8e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(filtered=False, sample=False):\n",
    "    filename = \"filtered\" if filtered else \"unfiltered\"\n",
    "    all_csv_files = list(Path(\"../data\").glob(\"*\"))\n",
    "    csv_file = [csv for csv in all_csv_files if filename in csv.name][0]\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print()\n",
    "\n",
    "    if sample:\n",
    "        print(\"Using sampled DF\")\n",
    "        df = df.sample(frac=0.2)\n",
    "        df = df.loc[(df!=0).any(axis=1)]\n",
    "    print(\"df shape:\", df.shape)\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c10facc-e91a-4771-8658-ed1f7aa96832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_splits(df, filtered=False, test_size=0.2):\n",
    "    col_names = list(df.columns)\n",
    "    split_fn = get_stratified_splits if filtered else get_nonstratified_splits\n",
    "    (X_train, X_test, X_val, y_train, y_test, y_val) = split_fn(df, test_size=test_size)\n",
    "    train_df = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "    test_df = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    val_df = pd.merge(X_val, y_val, left_index=True, right_index=True)\n",
    "    \n",
    "    train_df.columns = col_names\n",
    "    test_df.columns = col_names\n",
    "    val_df.columns = col_names\n",
    "\n",
    "    return train_df, test_df, val_df\n",
    "    \n",
    "def get_stratified_splits(df, test_size=0.2):\n",
    "    columns = list(df.columns)\n",
    "    X = df.filename.to_frame().to_numpy()\n",
    "    y = df.drop(columns=[\"filename\"]).to_numpy()\n",
    "\n",
    "    X_train, y_train, X_test_val, y_test_val = iterative_train_test_split(X, y, test_size=0.2)\n",
    "    X_test, y_test, X_val, y_val = iterative_train_test_split(X_test_val, y_test_val, test_size=0.5)\n",
    "    datasets = (X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "    datasets = [pd.DataFrame(dataset) for dataset in datasets]\n",
    "    return tuple(datasets)\n",
    "\n",
    "def get_nonstratified_splits(df, test_size=0.2):\n",
    "    X = df.filename\n",
    "    y = df.drop(columns=[\"filename\"])\n",
    "    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=SEED)\n",
    "\n",
    "    return (X_train, X_test, X_val, y_train, y_test, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b408146a-c6b0-4f73-94b1-e49a828aa3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(filename, img_size):\n",
    "    filepath = str(image_dir) + \"/\" + filename\n",
    "    img = tf.io.read_file(filepath)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img\n",
    "    \n",
    "def process_dataset(filename, labels, img_size):\n",
    "    img = decode_img(filename, img_size=img_size)\n",
    "    return img, labels\n",
    "\n",
    "def configure_datasets_for_performance(datasets, shuffle=False, batch_size=BATCH_SIZE):\n",
    "    configured_datasets = []\n",
    "    for dataset in datasets:\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=dataset.cardinality(), reshuffle_each_iteration=True)\n",
    "        dataset = dataset.batch(batch_size=batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        configured_datasets.append(dataset)\n",
    "    return tuple(configured_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e76af0-d22b-44d6-975e-625de5e042f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_from_dataframes(img_size, splits=None):\n",
    "    datasets = []\n",
    "    for split in splits:\n",
    "        img = split.filename\n",
    "        labels = split.drop(columns=[\"filename\"])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((img, labels))\n",
    "        dataset = dataset.map(lambda x,y: process_dataset(x,y, img_size))\n",
    "        datasets.append(dataset)\n",
    "    return tuple(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba6b97a-0aab-435c-98c4-362be55d13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(img_size, df, test_size=0.2):\n",
    "    splits = get_train_test_splits(df, test_size=test_size)\n",
    "    datasets = datasets_from_dataframes(img_size, splits=splits)\n",
    "    for dataset in zip([\"train\", \"val\", \"test\"], datasets):\n",
    "        print(dataset[0],\"size:\", len(dataset[1]))\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515feb0-4819-43c6-8a91-06e8ab3f21c8",
   "metadata": {},
   "source": [
    "# CREATE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46648f09-b30c-4ec3-bd2e-ee4510403bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freeze_at_layer(model_name):\n",
    "    model_name = model_name.lower()\n",
    "    if \"mobilenet\" in model_name:\n",
    "        # 120 used in FP\n",
    "        # last block 143\n",
    "        return 120\n",
    "    if \"vgg16\" in model_name:\n",
    "        # 14 used in FP\n",
    "        # last layer 17\n",
    "        return 14\n",
    "    if \"inceptionv3\" in model_name:\n",
    "        # 172 used in FP\n",
    "        # last block 279\n",
    "        return 172\n",
    "    if \"inceptionresnetv2\" in model_name:\n",
    "        # 516 used in FP\n",
    "        # 761 last block\n",
    "        return 516\n",
    "\n",
    "    # Function to freeze the given base model at the desired layer number\n",
    "def freeze_model(base_model, freeze_at):\n",
    "    print(f\"Unfreezing from {freeze_at}.\")\n",
    "    # freezes initial layers\n",
    "    for layer in base_model.layers[:freeze_at]:\n",
    "        layer.trainable=False\n",
    "    # unfreezes later\n",
    "    for layer in base_model.layers[freeze_at:]:\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53118a56-c690-4d12-b276-22e2c2f6ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, loss=None, metrics=[], fine_tuning=False):\n",
    "    opt = keras.optimizers.Adam\n",
    "    lr = (float(opt().learning_rate) / 10) if fine_tuning else float(opt().learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr, gradient_accumulation_steps=4),\n",
    "        metrics=metrics\n",
    "    )\n",
    "    print_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef3b3a23-a3bf-49c8-8941-2fc544763b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=None, train_ds=None, val_ds=None, epochs=100, loss=None, metrics=[], fine_tuning=False, initial_epoch=0):\n",
    "    if fine_tuning:\n",
    "        freeze_layer = get_freeze_at_layer(model.name)\n",
    "        freeze_model(model, freeze_layer)\n",
    "        \n",
    "    compile_model(model, loss=loss, metrics=metrics, fine_tuning=fine_tuning)\n",
    "    \n",
    "    string_suffix = \"FT\" if fine_tuning else \"CLF\"\n",
    "    print(\"Training -\", string_suffix)\n",
    "   \n",
    "    cbs = get_callbacks(model.name, fine_tuning=fine_tuning)\n",
    "\n",
    "    print(f\"Starting training at epoch {initial_epoch}.\" )\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=cbs, initial_epoch=initial_epoch)\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df['model'] = model.name\n",
    "    history_df['epoch'] = history.epoch\n",
    "    history_df['type'] = string_suffix\n",
    "    display(history_df)\n",
    "    history_df.to_csv(str(histories_dir) + \"/\" + model.name + string_suffix + \".csv\", index=False)  \n",
    "    \n",
    "    best_epoch = history_df.loc[history_df.val_loss == history_df.val_loss.min(), \"epoch\"].values[0] + 1\n",
    "    print(f\"Best epoch number: {best_epoch}\")\n",
    "    return best_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eea7cf9-d314-4cfa-8f2b-6efd5d4bd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(epochs=5, sample=True):\n",
    "    i = 0\n",
    "    num_models = 0\n",
    "\n",
    "   # app_names = [\"mobilenet_v2\", \"vgg16\", \"inception_v3\", \"inception_resnet_v2\"]\n",
    "    app_names = [\"inception_resnet_v2\"]\n",
    "    \n",
    "  #  methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    methodologies = [\"multiclass\"]\n",
    "\n",
    "   # filter_options = [False, True]\n",
    "    filter_options = [True]\n",
    "\n",
    "\n",
    "    # Get total count of models to be training\n",
    "    for filtered in filter_options:\n",
    "        for methodology in methodologies:\n",
    "            if not filtered and (methodology == \"multilabel\"):\n",
    "                continue\n",
    "            for app_name in app_names:\n",
    "                num_models += 1\n",
    "\n",
    "    # Training Loop\n",
    "    for filtered in filter_options:\n",
    "        for methodology in methodologies:\n",
    "            if not filtered and (methodology == \"multilabel\"):\n",
    "                continue\n",
    "                \n",
    "            loss, activation, metrics = get_hyperparameters(methodology)\n",
    "    \n",
    "            df = get_dataframe(filtered=filtered, sample=sample)\n",
    "            num_classes = len(df.columns[1:])\n",
    "            \n",
    "            for app_name in app_names:\n",
    "                i += 1\n",
    "                print()\n",
    "                print(f\"Model: {i} of {num_models}\")\n",
    "                print(\"Filtered dataset:\", filtered)\n",
    "                print(\"Methodology:\", methodology)\n",
    "       \n",
    "                model = build_model(app_name, activation=activation, num_classes=num_classes)\n",
    "\n",
    "                img_size = model.input_shape[1:3]\n",
    "                \n",
    "                datasets = get_datasets(img_size, df=df)\n",
    "                train_ds, test_ds, val_ds = configure_datasets_for_performance(datasets)\n",
    "\n",
    "                model.name = model.name + \"_\" + (\"filtered\" if filtered else \"unfiltered\") + \"_\" + methodology\n",
    "                print(\"Save file name:\", model.name)\n",
    "\n",
    "                # Train top Classifier\n",
    "                best_epoch = train_model(model=model, train_ds=train_ds, val_ds=val_ds, epochs=epochs, loss=loss, metrics=metrics, fine_tuning=False, initial_epoch=0)\n",
    "                # Fine Tuning\n",
    "                ft_start_epoch = best_epoch + 1\n",
    "                train_model(model=model, train_ds=train_ds, val_ds=val_ds, epochs=epochs, loss=loss, metrics=metrics, fine_tuning=True, initial_epoch=ft_start_epoch)\n",
    "\n",
    "                del model\n",
    "                gc.collect()\n",
    "                tensorflow.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f912e95-e514-4310-8ead-fad2a7b817b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df shape: (5154, 28)\n",
      "\n",
      "Model: 1 of 1\n",
      "Filtered dataset: True\n",
      "Methodology: multiclass\n",
      "train size: 4123\n",
      "val size: 515\n",
      "test size: 516\n",
      "Save file name: inception_resnet_v2_filtered_multiclass\n",
      "Model name: inception_resnet_v2_filtered_multiclass\n",
      "Input shape: (None, 299, 299, 3)\n",
      "Optimizer name: adam learning_rate: 0.001\n",
      "Loss: categorical_crossentropy\n",
      "Metrics:\n",
      "categorical_accuracy\n",
      "{'name': 'f1_score_weighted', 'dtype': 'float32', 'average': 'weighted', 'threshold': None}\n",
      "{'name': 'f1_score_per_class', 'dtype': 'float32', 'average': None, 'threshold': None}\n",
      "Classifier layer activation function: softmax\n",
      "\n",
      "Training - CLF\n",
      "Starting training at epoch 0.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744116915.919498   34917 service.cc:152] XLA service 0x7fb2e00b0380 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744116915.919568   34917 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-04-08 13:55:16.918849: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1744116920.070982   34917 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1744116931.257708   34917 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - categorical_accuracy: 0.4916 - f1_score_per_class: 0.0981 - f1_score_weighted: 0.4081 - loss: 1.9857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 13:56:20.387009: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6134', 180 bytes spill stores, 180 bytes spill loads\n",
      "\n",
      "2025-04-08 13:56:29.452302: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6134', 180 bytes spill stores, 180 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.14157, saving model to ../saved_models/inception_resnet_v2_filtered_multiclass.keras\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 135ms/step - categorical_accuracy: 0.4918 - f1_score_per_class: 0.0982 - f1_score_weighted: 0.4083 - loss: 1.9849 - val_categorical_accuracy: 0.6725 - val_f1_score_per_class: 0.1830 - val_f1_score_weighted: 0.5942 - val_loss: 1.1416\n",
      "Epoch 2/100\n",
      "\u001b[1m515/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.6943 - f1_score_per_class: 0.2579 - f1_score_weighted: 0.6364 - loss: 1.0254\n",
      "Epoch 2: val_loss improved from 1.14157 to 0.99607, saving model to ../saved_models/inception_resnet_v2_filtered_multiclass.keras\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 55ms/step - categorical_accuracy: 0.6943 - f1_score_per_class: 0.2580 - f1_score_weighted: 0.6364 - loss: 1.0253 - val_categorical_accuracy: 0.6996 - val_f1_score_per_class: 0.2532 - val_f1_score_weighted: 0.6454 - val_loss: 0.9961\n",
      "Epoch 3/100\n",
      "\u001b[1m234/516\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 44ms/step - categorical_accuracy: 0.7319 - f1_score_per_class: 0.3243 - f1_score_weighted: 0.6876 - loss: 0.8525"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mrun_all\u001b[39m\u001b[34m(epochs, sample)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSave file name:\u001b[39m\u001b[33m\"\u001b[39m, model.name)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Train top Classifier\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m best_epoch = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tuning\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Fine Tuning\u001b[39;00m\n\u001b[32m     54\u001b[39m ft_start_epoch = best_epoch + \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_ds, val_ds, epochs, loss, metrics, fine_tuning, initial_epoch)\u001b[39m\n\u001b[32m     11\u001b[39m cbs = get_callbacks(model.name, fine_tuning=fine_tuning)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m history_df = pd.DataFrame(history.history)\n\u001b[32m     16\u001b[39m history_df[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m] = model.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m     iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m ):\n\u001b[32m    219\u001b[39m     opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs.get_value()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.12/site-packages/tensorflow/python/data/ops/optional_ops.py:176\u001b[39m, in \u001b[36m_OptionalImpl.has_value\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.12/site-packages/tensorflow/python/ops/gen_optional_ops.py:172\u001b[39m, in \u001b[36moptional_has_value\u001b[39m\u001b[34m(optional, name)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    171\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOptionalHasValue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = run_all(epochs=10, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42275b9b-0807-441e-ab6e-9a33b466007e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
