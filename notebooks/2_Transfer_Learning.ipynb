{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ed7a41-d76f-4485-bd2d-4585b67de3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 16:01:21.319182: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744038081.335035   12437 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744038081.339565   12437 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744038081.352442   12437 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744038081.352470   12437 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744038081.352472   12437 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744038081.352474   12437 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-07 16:01:21.356875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.applications as apps\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7199ab32-2dd9-4954-89bc-f79724d31950",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "BASE_LR = 0.01\n",
    "PATIENCE = 10\n",
    "saved_models_dir = Path(\"../save_models\")\n",
    "saved_models_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8acee17-a6f9-4b36-99c3-ceb2495b8991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPU, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744038086.720445   12437 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def set_memory_growth():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "      except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "def set_memory_limit(memory_limit):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=memory_limit)]\n",
    "        )\n",
    "\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "set_memory_limit(4096)\n",
    "#set_memory_growth()\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd067d-0574-40b9-8056-6f005779c464",
   "metadata": {},
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603e5f2a-3699-43cb-8a37-230ab0540cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ruairi/.cache/kagglehub/datasets/bloox2/fieldplant/versions/1/train\n"
     ]
    }
   ],
   "source": [
    "# Download latest version of data\n",
    "image_dir = kagglehub.dataset_download(\"bloox2/fieldplant\")\n",
    "image_dir = Path(image_dir) / \"train\"\n",
    "print(\"Path to dataset files:\", image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62d7c5-a1aa-41ca-94bc-6a77bc736d81",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2498b5-beb6-4719-9187-aad8c0b1ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_preprocessing(app_name):\n",
    "    app = getattr(keras.applications, app_name)\n",
    "    model_name = dir(app)[0]\n",
    "    model = getattr(app, model_name)\n",
    "    input_shape = model().input_shape[1:]\n",
    "    model = model(include_top=False, input_shape=input_shape)\n",
    "    model.trainable = False\n",
    "    preprocessing = getattr(app, \"preprocess_input\")\n",
    "    return model, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c41c77b0-169c-482a-85a8-cbf086a33f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(app_name, activation, num_classes):\n",
    "    model, preprocessing = get_model_and_preprocessing(app_name)\n",
    "\n",
    "    inputs = keras.Input(shape=model.input_shape[1:])\n",
    "    x = preprocessing(inputs)\n",
    "    x = model(x, training=False)\n",
    "    \n",
    "    outputs = keras.layers.Dense(num_classes, activation=activation, name=\"classifier_layer\")(x)\n",
    "\n",
    "    model_name = model.name\n",
    "    model = keras.Model(inputs, outputs, name=model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5128bc-fe08-46fe-8f41-6c5705e4d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(methodology):\n",
    "    multilabel = methodology == \"multilabel\"\n",
    "    \n",
    "    methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    losses = [\"categorical_crossentropy\", \"binary_crossentropy\"]\n",
    "    activation = [\"softmax\", \"sigmoid\"]\n",
    "    metrics = [\"categorical_accuracy\", \"binary_crossentropy\"]\n",
    "\n",
    "    idx = methodologies.index(methodology)\n",
    "\n",
    "    metrics = [metrics[idx]]\n",
    "    f1_score_weighted = keras.metrics.F1Score(average=\"weighted\", threshold=0.5 if multilabel else None, name=\"f1_score_weighted\", dtype=None)\n",
    "    f1_score_per_class = keras.metrics.F1Score(average=None, threshold=0.5 if multilabel else None, name=\"f1_score_per_class\", dtype=None)\n",
    "    metrics.append(f1_score_weighted)\n",
    "    metrics.append(f1_score_per_class)\n",
    "        \n",
    "    hyperparams = [losses[idx], activation[idx], metrics]\n",
    "    \n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52871059-9ca3-435f-9cf0-d738f597d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model):\n",
    "    compile_config = model._compile_config.config\n",
    "    optimizer = compile_config['optimizer'].get_config()\n",
    "    classifier_activation = model.get_layer(name=\"classifier_layer\").activation.__name__\n",
    "\n",
    "    print(\"Model name:\", model.name)\n",
    "    print(\"Input shape:\", model.input_shape)\n",
    "    print(\"Optimizer name:\", optimizer['name'], \"learning_rate:\", np.round(optimizer['learning_rate'], 6))\n",
    "    print(\"Loss:\", compile_config['loss'])\n",
    "    print(\"Metrics:\", compile_config['metrics'])\n",
    "    print(\"Classifier layer activation function:\", classifier_activation)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08bb57e7-5137-4267-9bd0-fc7dc629e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name):\n",
    "    cbs = [\n",
    "        keras.callbacks.EarlyStopping(patience=PATIENCE, restore_best_weights=True, baseline=None, verbose=1),\n",
    "        keras.callbacks.ModelCheckpoint(filepath=f\"{saved_models_dir}{model_name}.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1, initial_value_threshold=None)]\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d421be4-0a10-4ad9-8877-ed16835789b6",
   "metadata": {},
   "source": [
    "# GET DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bcc6399-8c40-434a-b4ff-082e09a8e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(filtered=False):\n",
    "    filename = \"filtered\" if filtered else \"unfiltered\"\n",
    "    all_csv_files = list(Path(\"../data\").glob(\"*\"))\n",
    "    csv_file = [csv for csv in all_csv_files if filename in csv.name][0]\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c10facc-e91a-4771-8658-ed1f7aa96832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_splits(df, filtered=False, test_size=0.2):\n",
    "    col_names = list(df.columns)\n",
    "    split_fn = get_stratified_splits if filtered else get_nonstratified_splits\n",
    "    (X_train, X_test, X_val, y_train, y_test, y_val) = split_fn(df, test_size=test_size)\n",
    "    train_df = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "    test_df = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    val_df = pd.merge(X_val, y_val, left_index=True, right_index=True)\n",
    "    \n",
    "    train_df.columns = col_names\n",
    "    test_df.columns = col_names\n",
    "    val_df.columns = col_names\n",
    "\n",
    "    return train_df, test_df, val_df\n",
    "    \n",
    "def get_stratified_splits(df, test_size=0.2):\n",
    "    columns = list(df.columns)\n",
    "    X = df.filename.to_frame().to_numpy()\n",
    "    y = df.drop(columns=[\"filename\"]).to_numpy()\n",
    "\n",
    "    X_train, y_train, X_test_val, y_test_val = iterative_train_test_split(X, y, test_size=0.2)\n",
    "    X_test, y_test, X_val, y_val = iterative_train_test_split(X_test_val, y_test_val, test_size=0.5)\n",
    "    datasets = (X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "    datasets = [pd.DataFrame(dataset) for dataset in datasets]\n",
    "    return tuple(datasets)\n",
    "\n",
    "def get_nonstratified_splits(df, test_size=0.2):\n",
    "    X = df.filename\n",
    "    y = df.drop(columns=[\"filename\"])\n",
    "    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=SEED)\n",
    "\n",
    "    return (X_train, X_test, X_val, y_train, y_test, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b408146a-c6b0-4f73-94b1-e49a828aa3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(filename, img_size):\n",
    "    filepath = str(image_dir) + \"/\" + filename\n",
    "    img = tf.io.read_file(filepath)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img\n",
    "    \n",
    "def process_dataset(filename, labels, img_size):\n",
    "    img = decode_img(filename, img_size=img_size)\n",
    "    return img, labels\n",
    "\n",
    "def configure_dataset_for_performance(dataset, shuffle=False, batch_size=BATCH_SIZE):\n",
    "    dataset = dataset.cache()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=dataset.cardinality(), reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(batch_size=batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e76af0-d22b-44d6-975e-625de5e042f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_from_dataframes(img_size, splits=None):\n",
    "    datasets = []\n",
    "    for split in splits:\n",
    "        img = split.filename\n",
    "        labels = split.drop(columns=[\"filename\"])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((img, labels))\n",
    "        dataset = dataset.map(lambda x,y: process_dataset(x,y, img_size))\n",
    "        dataset = configure_dataset_for_performance(dataset)\n",
    "        datasets.append(dataset)\n",
    "    return tuple(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ba6b97a-0aab-435c-98c4-362be55d13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(img_size, filtered=False, test_size=0.2):\n",
    "    df = get_dataframe(filtered)\n",
    "    splits = get_train_test_splits(df, filtered=filtered, test_size=test_size)\n",
    "    datasets = datasets_from_dataframes(img_size, splits=splits)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515feb0-4819-43c6-8a91-06e8ab3f21c8",
   "metadata": {},
   "source": [
    "# CREATE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eea7cf9-d314-4cfa-8f2b-6efd5d4bd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(epochs=5):\n",
    "    i = 0\n",
    "    app_names = [\"mobilenet_v2\", \"vgg16\", \"inception_v3\", \"inception_resnet_v2\"]\n",
    "    #app_names = [\"mobilenet_v2\"]\n",
    "    methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    methodologies = [\"multiclass\"]\n",
    "\n",
    "\n",
    "    for filtered in [False, True]:\n",
    "        for methodology in methodologies:\n",
    "            if not filtered and (methodology == \"binary\"):\n",
    "                continue\n",
    "                \n",
    "            loss, activation, metrics = get_hyperparameters(methodology)\n",
    "    \n",
    "            for filtered in [False, True]:\n",
    "                df = get_dataframe(filtered=filtered)\n",
    "                num_classes = len(df.columns[1:])\n",
    "                \n",
    "                for app_name in app_names:\n",
    "                    i += 1\n",
    "                    print(\"Model:\", i)\n",
    "                    print(\"Filtered dataset:\", filtered)\n",
    "                    print(\"Methodology:\", methodology)\n",
    "           \n",
    "                    model = build_model(app_name, activation=activation, num_classes=num_classes)\n",
    "\n",
    "                    img_size = model.input_shape[1:3]\n",
    "                    train_ds, test_ds, val_ds = get_datasets(img_size, filtered=filtered)\n",
    "                \n",
    "                    model.compile(\n",
    "                        loss=loss,\n",
    "                        optimizer=keras.optimizers.Adam(learning_rate=BASE_LR),\n",
    "                        metrics=metrics\n",
    "                    )\n",
    "                    print_model_info(model)\n",
    "        \n",
    "                    cbs = get_callbacks(model.name)\n",
    "        \n",
    "                    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=cbs)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f912e95-e514-4310-8ead-fad2a7b817b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 1\n",
      "Filtered dataset: False\n",
      "Methodology: multiclass\n",
      "Model name: mobilenetv2_1.00_224\n",
      "Input shape: (None, 224, 224, 3)\n",
      "Optimizer name: adam learning_rate: 0.01\n",
      "Loss: categorical_crossentropy\n",
      "Metrics: ['categorical_accuracy', <F1Score name=f1_score_weighted>, <F1Score name=f1_score_per_class>]\n",
      "Classifier layer activation function: softmax\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None, 27), output.shape=(None, 7, 7, 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mrun_all\u001b[39m\u001b[34m(epochs)\u001b[39m\n\u001b[32m     36\u001b[39m print_model_info(model)\n\u001b[32m     38\u001b[39m cbs = get_callbacks(model.name)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:653\u001b[39m, in \u001b[36mcategorical_crossentropy\u001b[39m\u001b[34m(target, output, from_logits, axis)\u001b[39m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    648\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArguments `target` and `output` must be at least rank 1. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReceived: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m     )\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target.shape) != \u001b[38;5;28mlen\u001b[39m(output.shape):\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    654\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArguments `target` and `output` must have the same rank \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    655\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(ndim). Received: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    656\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    657\u001b[39m     )\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target.shape, output.shape):\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 != e2:\n",
      "\u001b[31mValueError\u001b[39m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None, 27), output.shape=(None, 7, 7, 27)"
     ]
    }
   ],
   "source": [
    "model = run_all(epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82894619-5d32-4ee0-8b21-2f3b9ad13db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc2393-cf32-4e27-931c-1427aadb6363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd159363-dfc3-4975-93da-7e693e08e810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
