{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ed7a41-d76f-4485-bd2d-4585b67de3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 11:40:21.808558: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744195221.875996   40196 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744195221.894614   40196 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744195222.019678   40196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744195222.019711   40196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744195222.019714   40196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744195222.019716   40196 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-09 11:40:22.040336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.applications as apps\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8acee17-a6f9-4b36-99c3-ceb2495b8991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPU, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744195228.126748   40196 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def set_memory_growth():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "      except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "def set_memory_limit(memory_limit):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=memory_limit)]\n",
    "        )\n",
    "\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "set_memory_limit(4096)\n",
    "#set_memory_growth()\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7199ab32-2dd9-4954-89bc-f79724d31950",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE_TOTAL = 128\n",
    "BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = int(BATCH_SIZE_TOTAL / BATCH_SIZE)\n",
    "PATIENCE = 5\n",
    "\n",
    "saved_models_dir = Path(\"../saved_models\")\n",
    "saved_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "histories_dir = Path(\"../histories\")\n",
    "histories_dir.mkdir(parents=True, exist_ok=True)\n",
    "logs_dir = Path(\"../logs\")\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd067d-0574-40b9-8056-6f005779c464",
   "metadata": {},
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603e5f2a-3699-43cb-8a37-230ab0540cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download latest version of data\n",
    "# Use the next 2 lines if not downloaded before\n",
    "image_dir = kagglehub.dataset_download(\"bloox2/fieldplant\")\n",
    "image_dir = Path(image_dir) / \"train\"\n",
    "\n",
    "# Use the next lines of code if your data has been downloaded already, but you are offline.  Will used cached data.\n",
    "# image_dir = \"~/.cache/kagglehub/datasets/bloox2/fieldplant/versions/1/train\"\n",
    "# print(\"Path to dataset files:\", image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62d7c5-a1aa-41ca-94bc-6a77bc736d81",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2498b5-beb6-4719-9187-aad8c0b1ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_preprocessing(app_name):\n",
    "    app = getattr(keras.applications, app_name)\n",
    "    model_name = dir(app)[0]\n",
    "    model = getattr(app, model_name)\n",
    "    input_shape = model().input_shape[1:]\n",
    "    model = model(include_top=False, input_shape=input_shape, weights=\"imagenet\")\n",
    "    model.trainable = False\n",
    "    preprocessing = getattr(app, \"preprocess_input\")\n",
    "    return model, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4db47e-824e-4444-8229-20c5ab4214d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_pre_classifier_layers(model_name):\n",
    "#     if \"vgg\" in model_name:\n",
    "#         return keras.Sequential([\n",
    "#             keras.layers.GlobalAveragePooling2D(),\n",
    "#             keras.layers.Dense(1024, activation=\"relu\"),\n",
    "#             keras.layers.Dense(1024, activation=\"relu\") ])\n",
    "#     else:\n",
    "#         return keras.Sequential([keras.layers.GlobalAveragePooling2D()])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af6e7f7-8031-4c32-9565-b38844a9ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_classifier_layers(model_name):\n",
    "    dropout_value = 0.2\n",
    "    \n",
    "    if 'vgg' in model_name:\n",
    "        return keras.Sequential([\n",
    "            keras.layers.GlobalAveragePooling2D(),\n",
    "            keras.layers.Dense(4096, activation='relu'),\n",
    "            keras.layers.Dense(1072, activation='relu'),\n",
    "            keras.layers.Dropout(dropout_value)\n",
    "        ])\n",
    "    else:\n",
    "        seq = keras.Sequential()\n",
    "        seq.add(keras.layers.GlobalAveragePooling2D())\n",
    "        if \"mobilenet\" not in model_name:\n",
    "            seq.add(keras.layers.Dense(1024, activation = 'relu'))\n",
    "        seq.add(keras.layers.Dropout(dropout_value))\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41c77b0-169c-482a-85a8-cbf086a33f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(app_name, activation, num_classes):\n",
    "    model, preprocessing = get_model_and_preprocessing(app_name)\n",
    "    pre_classifier_layers = get_pre_classifier_layers(model.name)\n",
    "\n",
    "    inputs = keras.Input(shape=model.input_shape[1:])\n",
    "    x = preprocessing(inputs)\n",
    "    x = model(x, training=False)\n",
    "    x = pre_classifier_layers(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation=activation, name=\"classifier_layer\")(x)\n",
    "\n",
    "    model_name = model.name\n",
    "    model = keras.Model(inputs, outputs, name=model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5128bc-fe08-46fe-8f41-6c5705e4d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(methodology):\n",
    "    multilabel = methodology == \"multilabel\"\n",
    "    \n",
    "    methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    losses = [\"categorical_crossentropy\", \"binary_crossentropy\"]\n",
    "    activation = [\"softmax\", \"sigmoid\"]\n",
    "    metrics = [\"categorical_accuracy\", \"binary_accuracy\"]\n",
    "\n",
    "    idx = methodologies.index(methodology)\n",
    "\n",
    "    metrics = [metrics[idx]]\n",
    "    f1_score_weighted = keras.metrics.F1Score(average=\"weighted\", threshold=0.5 if multilabel else None, name=\"f1_score_weighted\", dtype=None)\n",
    "    f1_score_per_class = keras.metrics.F1Score(average=None, threshold=0.5 if multilabel else None, name=\"f1_score_per_class\", dtype=None)\n",
    "    metrics.append(f1_score_weighted)\n",
    "    metrics.append(f1_score_per_class)\n",
    "        \n",
    "    hyperparams = [losses[idx], activation[idx], metrics]\n",
    "    \n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52871059-9ca3-435f-9cf0-d738f597d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model):\n",
    "    print()\n",
    "    print(\"print_model_info() start\")\n",
    "    compile_config = model._compile_config.config\n",
    "    optimizer = compile_config['optimizer'].get_config()\n",
    "    classifier_activation = model.get_layer(name=\"classifier_layer\").activation.__name__\n",
    "\n",
    "    print(\"Model name:\", model.name)\n",
    "    print(\"Input shape:\", model.input_shape)\n",
    "    print(\"Optimizer name:\", optimizer['name'], \"learning_rate:\", np.round(optimizer['learning_rate'], 6))\n",
    "    print(\"Loss:\", compile_config['loss'])\n",
    "    print(\"Metrics:\")\n",
    "    for metric in compile_config['metrics']:\n",
    "        print(metric if isinstance(metric, str) else metric.get_config())\n",
    "    print(\"Classifier layer activation function:\", classifier_activation)\n",
    "    print()\n",
    "    print(model.summary(expand_nested=True, show_trainable=True))\n",
    "    print()\n",
    "\n",
    "    print(\"print_model_info() end\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08bb57e7-5137-4267-9bd0-fc7dc629e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(best_epoch=None, model_name=None, fine_tuning=False):\n",
    "    if fine_tuning:\n",
    "        baseline_loss = best_epoch.val_loss.values[0]\n",
    "        print(\"Previous best epoch (starting counting from 0):\", best_epoch.epoch.values[0])\n",
    "        print(\"Previous best epoch (starting counting from 1, as per training loop):\", best_epoch.epoch.values[0] +1)\n",
    "\n",
    "        print(\"Baseline val_loss:\", baseline_loss)\n",
    "    cbs = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            patience=PATIENCE, restore_best_weights=True,\n",
    "            baseline=None if not fine_tuning else baseline_loss,\n",
    "            verbose=1),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"{saved_models_dir}/{model_name}.keras\",\n",
    "            save_best_only=True, monitor=\"val_loss\",\n",
    "            verbose=1,\n",
    "            initial_value_threshold=None if not fine_tuning else baseline_loss\n",
    "        )]\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d421be4-0a10-4ad9-8877-ed16835789b6",
   "metadata": {},
   "source": [
    "# GET DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bcc6399-8c40-434a-b4ff-082e09a8e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(filtered=False, sample=False):\n",
    "    filename = \"filtered\" if filtered else \"unfiltered\"\n",
    "    all_csv_files = list(Path(\"../data\").glob(\"*\"))\n",
    "    csv_file = [csv for csv in all_csv_files if filename in csv.name][0]\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print()\n",
    "\n",
    "    if sample:\n",
    "        print(\"Using sampled DF\")\n",
    "        df = df.sample(frac=0.2)\n",
    "        df = df.loc[(df!=0).any(axis=1)]\n",
    "    print(\"Df shape:\", df.shape)\n",
    "    print()\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c10facc-e91a-4771-8658-ed1f7aa96832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_splits(df, filtered=False, test_size=0.2):\n",
    "    col_names = list(df.columns)\n",
    "    split_fn = get_stratified_splits if filtered else get_nonstratified_splits\n",
    "    (X_train, X_test, X_val, y_train, y_test, y_val) = split_fn(df, test_size=test_size)\n",
    "    train_df = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "    test_df = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    val_df = pd.merge(X_val, y_val, left_index=True, right_index=True)\n",
    "    \n",
    "    train_df.columns = col_names\n",
    "    test_df.columns = col_names\n",
    "    val_df.columns = col_names\n",
    "\n",
    "    return train_df, test_df, val_df\n",
    "    \n",
    "def get_stratified_splits(df, test_size=0.2):\n",
    "    columns = list(df.columns)\n",
    "    X = df.filename.to_frame().to_numpy()\n",
    "    y = df.drop(columns=[\"filename\"]).to_numpy()\n",
    "\n",
    "    X_train, y_train, X_test_val, y_test_val = iterative_train_test_split(X, y, test_size=0.2)\n",
    "    X_test, y_test, X_val, y_val = iterative_train_test_split(X_test_val, y_test_val, test_size=0.5)\n",
    "    datasets = (X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "    datasets = [pd.DataFrame(dataset) for dataset in datasets]\n",
    "    return tuple(datasets)\n",
    "\n",
    "def get_nonstratified_splits(df, test_size=0.2):\n",
    "    X = df.filename\n",
    "    y = df.drop(columns=[\"filename\"])\n",
    "    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=SEED)\n",
    "\n",
    "    return (X_train, X_test, X_val, y_train, y_test, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b408146a-c6b0-4f73-94b1-e49a828aa3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(filename, img_size):\n",
    "    filepath = str(image_dir) + \"/\" + filename\n",
    "    img = tf.io.read_file(filepath)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img\n",
    "    \n",
    "def process_dataset(filename, labels, img_size):\n",
    "    img = decode_img(filename, img_size=img_size)\n",
    "    return img, labels\n",
    "\n",
    "def configure_datasets_for_performance(datasets, shuffle=False, batch_size=BATCH_SIZE):\n",
    "    configured_datasets = []\n",
    "    ds_sizes = [int(ds.cardinality().numpy()) for ds in datasets]\n",
    "    print(\"Ds sizes:\", ds_sizes)\n",
    "    print(\"NP argmax:\", np.argmax(ds_sizes))\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        if int(dataset.cardinality().numpy()) == ds_sizes[np.argmax(ds_sizes)]:\n",
    "            print(f\"Shuffling dataset {i}\")\n",
    "            print()\n",
    "            dataset = dataset.shuffle(buffer_size=dataset.cardinality(), reshuffle_each_iteration=True)\n",
    "        dataset = dataset.batch(batch_size=batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        configured_datasets.append(dataset)\n",
    "    return tuple(configured_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8e76af0-d22b-44d6-975e-625de5e042f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_from_dataframes(img_size, splits=None):\n",
    "    datasets = []\n",
    "    ds_names = [\"train\", \"val\", \"test\"]\n",
    "    for i, split in enumerate(splits):\n",
    "        img = split.filename\n",
    "        labels = split.drop(columns=[\"filename\"])\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((img, labels))\n",
    "        print(ds_names[i], \"length:\", len(dataset))\n",
    "\n",
    "        dataset = dataset.map(lambda x,y: process_dataset(x,y, img_size))\n",
    "        print(ds_names[i], dataset.element_spec)\n",
    "        print()\n",
    "        \n",
    "        datasets.append(dataset)\n",
    "    return tuple(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba6b97a-0aab-435c-98c4-362be55d13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(img_size, df, test_size=0.2):\n",
    "    splits = get_train_test_splits(df, test_size=test_size)\n",
    "    datasets = datasets_from_dataframes(img_size, splits=splits)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515feb0-4819-43c6-8a91-06e8ab3f21c8",
   "metadata": {},
   "source": [
    "# CREATE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46648f09-b30c-4ec3-bd2e-ee4510403bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    print()\n",
    "    print(\"freeze_model() start\")\n",
    "\n",
    "    base_layer_name = \"\"\n",
    "    num = 0\n",
    "    if \"mobilenet\" in model.name:\n",
    "        base_layer_name = \"mobilenetv2_1.00_224\"\n",
    "        num = 120\n",
    "    elif \"vgg16\" in model.name:\n",
    "        base_layer_name = \"vgg16\"\n",
    "        num = 14\n",
    "    elif \"inception_v3\" in model.name:\n",
    "        base_layer_name = \"inception_v3\"\n",
    "        num = 172\n",
    "    elif \"resnet\" in model.name:\n",
    "        base_layer_name = \"inception_resnet_v2\"\n",
    "        num = 516\n",
    "\n",
    "    print(f\"Freeze layers up to layer {num}\")\n",
    "    for layer in model.get_layer(name=base_layer_name).layers[:num]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.get_layer(name=base_layer_name).layers[num:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    print(\"freeze_model() end\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef3b3a23-a3bf-49c8-8941-2fc544763b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(best_epoch=None, model=None, train_ds=None, val_ds=None, epochs=100, loss=None, metrics=[], fine_tuning=False):\n",
    "    print()\n",
    "    print(\"train_model() start\")\n",
    "\n",
    "    string_suffix = \"FT\" if fine_tuning else \"CLF\"\n",
    "    print(\"Training -\", string_suffix)\n",
    "\n",
    "    if fine_tuning:\n",
    "        freeze_model(model)\n",
    "        \n",
    "    compile_model(model, loss=loss, metrics=metrics, fine_tuning=fine_tuning)\n",
    "    print_model_info(model)\n",
    "       \n",
    "    cbs = get_callbacks(model_name=model.name, fine_tuning=fine_tuning, best_epoch=best_epoch)\n",
    "\n",
    "    # +1 to best epoch so that it matches the number given in the training loop.  (So best=7 becomes best=8)\n",
    "    # +1 again so that it begins training from the epoch following the previous best. (So inital_epoch=8 becomes inital_epoch=9)\n",
    "    initial_epoch = 0 if not fine_tuning else best_epoch.epoch.values[0] + 1 + 1\n",
    "\n",
    "    print(f\"Starting training at epoch {initial_epoch}. (starting counting from 0, as per df)\" )\n",
    "    print(f\"Starting training at epoch {initial_epoch + 1}. (starting counting from 1, as per training loop)\" )\n",
    "    print()\n",
    "\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=cbs, initial_epoch=initial_epoch)\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df['model'] = model.name\n",
    "    history_df['epoch'] = history.epoch\n",
    "    history_df['type'] = string_suffix\n",
    "    \n",
    "    save_filename = f\"{str(histories_dir)}/{model.name}_{string_suffix}.csv\"\n",
    "    print()\n",
    "    print(\"Saving file as:\", save_filename)\n",
    "    history_df.to_csv(save_filename, index=False)  \n",
    "    \n",
    "    best_epoch = history_df.loc[history_df.val_loss == history_df.val_loss.min()]\n",
    "    print(f\"Best epoch number (starting from 0, as per df): {best_epoch.epoch.values[0]}\")\n",
    "    print(f\"Best epoch number (starting from 1, as per training loop): {best_epoch.epoch.values[0] + 1}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    display(history_df)\n",
    "\n",
    "    print()\n",
    "    print(\"train_model() end\")\n",
    "\n",
    "    return best_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53118a56-c690-4d12-b276-22e2c2f6ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, loss=None, metrics=[], fine_tuning=False):\n",
    "    print()\n",
    "    print(\"compile_model() start\")\n",
    "    opt = keras.optimizers.Adam\n",
    "    lr = (float(opt().learning_rate) / 10) if fine_tuning else float(opt().learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr, gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS),\n",
    "        metrics=metrics\n",
    "    )\n",
    "    print(\"compile_model() end\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eea7cf9-d314-4cfa-8f2b-6efd5d4bd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(epochs=5, sample=True, model_name=None):\n",
    "\n",
    "    print()\n",
    "    print(\"run_all() start\")\n",
    "\n",
    "    i = 0\n",
    "    num_models = 0\n",
    "\n",
    "    if model_name == \"all\":\n",
    "        app_names = [\"mobilenet_v2\", \"vgg16\", \"inception_v3\", \"inception_resnet_v2\"]\n",
    "    else:\n",
    "        app_names = [model_name]\n",
    "    methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    filter_options = [False, True]\n",
    "    \n",
    "\n",
    "    # # # Use for smaller combinations\n",
    "    # app_names = [\"vgg16\"]\n",
    "    # methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    # filter_options = [False, True]\n",
    "\n",
    "\n",
    "    # Get total count of models to be training\n",
    "    for filtered in filter_options:\n",
    "        for methodology in methodologies:\n",
    "            if not filtered and (methodology == \"multilabel\"):\n",
    "                continue\n",
    "            for app_name in app_names:\n",
    "                num_models += 1\n",
    "\n",
    "    # Training Loop\n",
    "    for filtered in filter_options:\n",
    "        for methodology in methodologies:\n",
    "            if not filtered and (methodology == \"multilabel\"):\n",
    "                continue\n",
    "                \n",
    "            loss, activation, metrics = get_hyperparameters(methodology)\n",
    "    \n",
    "            df = get_dataframe(filtered=filtered, sample=sample)\n",
    "            num_classes = len(df.columns[1:])\n",
    "            \n",
    "            for app_name in app_names:\n",
    "                i += 1\n",
    "                model = build_model(app_name, activation=activation, num_classes=num_classes)\n",
    "                model.name = model.name + \"_\" + (\"filtered\" if filtered else \"unfiltered\") + \"_\" + methodology\n",
    "                img_size = model.input_shape[1:3]\n",
    "                datasets = get_datasets(img_size, df=df)\n",
    "                train_ds, test_ds, val_ds = configure_datasets_for_performance(datasets)\n",
    "\n",
    "                  # Train top Classifier\n",
    "                for fine_tuning in [False, True]:\n",
    "                    print()\n",
    "                    print(f\"Model: {i} of {num_models}\")\n",
    "                    print(\"app_name:\", app_name)\n",
    "                    print(\"filtered:\", filtered)\n",
    "                    print(\"methodology:\", methodology)\n",
    "                    \n",
    "                    best_epoch = train_model(model=model, best_epoch=None if not fine_tuning else best_epoch,\n",
    "                                             train_ds=train_ds, val_ds=val_ds,\n",
    "                                             epochs=epochs, loss=loss, metrics=metrics,\n",
    "                                             fine_tuning=fine_tuning)\n",
    "\n",
    "                                   \n",
    "                # Clear memory\n",
    "                del model\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                print(); print(); print(); print();\n",
    "                print(\"BEGINNING NEXT MODEL IF EXISTS...\")\n",
    "\n",
    "                # # Train top Classifier\n",
    "                # best_epoch = train_model(model=model, train_ds=train_ds, val_ds=val_ds, epochs=epochs, loss=loss, metrics=metrics, fine_tuning=False)\n",
    "                # # Fine Tuning\n",
    "                # train_model(best_epoch=best_epoch, model=model, train_ds=train_ds, val_ds=val_ds, epochs=epochs, loss=loss, metrics=metrics, fine_tuning=True)\n",
    "\n",
    "                \n",
    "    print(\"run_all() end\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d64e3c-71fd-4109-aae0-3f6112ffe4c8",
   "metadata": {},
   "source": [
    "# RUN THE PROGRAMME\n",
    "The cell below runs the entire programme ddesigned above.<br>\n",
    "Change the parameters below as needed.\n",
    "\n",
    "epochs:  Choose num epochs, however EarlyStopping will likely stop before this.<br>\n",
    "sample: Choose whether to use a sample subset of the data.<br>\n",
    "print_to_file (not necessary): Choose whether to print the cells output to a file in /logs_dir, or print to cell output on screen.  If printing to file, you will not be able to see the cell output on screen as the programme runs, and so cannot track progress.<br><br>\n",
    "\n",
    "The previous cell function run_all(), can be altered to run all combinations of models, methodologies, and filtered options.  Or can run with any combination of these variables as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec2d6aa4-8c09-4521-b1f2-e6c95f2eee00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744195239.952031   40283 service.cc:152] XLA service 0x7f7f840516e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744195239.952070   40283 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-04-09 11:40:40.051251: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1744195240.593916   40283 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1744195247.192404   40283 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-04-09 11:40:50.620712: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1389', 444 bytes spill stores, 464 bytes spill loads\n",
      "\n",
      "2025-04-09 11:42:35.146982: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1229', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-04-09 11:42:35.242264: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1229', 700 bytes spill stores, 692 bytes spill loads\n",
      "\n",
      "2025-04-09 11:42:35.491961: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1229', 1588 bytes spill stores, 1584 bytes spill loads\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "epochs = 100\n",
    "sample = False\n",
    "print_to_file = True\n",
    "\n",
    "model_name = \"all\"\n",
    "# \"mobilenet_v2\", \"vgg16\", \"inception_v3\", \"inception_resnet_v2\"\n",
    "#model_name = \"mobilenet_v2\"\n",
    "\n",
    "\n",
    "if print_to_file:\n",
    "    with capture_output() as captured_output:\n",
    "        model = run_all(epochs=epochs, sample=sample, model_name=model_name)\n",
    "else:\n",
    "    model = run_all(epochs=epochs, sample=sample, model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "020372c9-e678-44ac-bbd6-1e5629298794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving log to: ../logs/mobilenet_v2.txt\n"
     ]
    }
   ],
   "source": [
    "save_log_filename = f\"{logs_dir}/{model_name}.txt\"\n",
    "print(\"Saving log to:\", save_log_filename)\n",
    "with open(save_log_filename, \"w\") as f:\n",
    "    f.write(captured_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28c6cf-eac2-4fbf-8844-161fd6528ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
