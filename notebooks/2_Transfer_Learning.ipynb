{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ed7a41-d76f-4485-bd2d-4585b67de3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 23:36:50.224866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744065410.244064   70721 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744065410.249672   70721 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744065410.266348   70721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744065410.266393   70721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744065410.266395   70721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744065410.266396   70721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-07 23:36:50.273092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.applications as apps\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7199ab32-2dd9-4954-89bc-f79724d31950",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 5\n",
    "\n",
    "saved_models_dir = Path(\"../saved_models\")\n",
    "saved_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "histories_dir = Path(\"../histories\")\n",
    "histories_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8acee17-a6f9-4b36-99c3-ceb2495b8991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPU, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744065413.532953   70721 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def set_memory_growth():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "      except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "def set_memory_limit(memory_limit):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=memory_limit)]\n",
    "        )\n",
    "\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "set_memory_limit(4096)\n",
    "#set_memory_growth()\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd067d-0574-40b9-8056-6f005779c464",
   "metadata": {},
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603e5f2a-3699-43cb-8a37-230ab0540cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ruairi/.cache/kagglehub/datasets/bloox2/fieldplant/versions/1/train\n"
     ]
    }
   ],
   "source": [
    "# Download latest version of data\n",
    "image_dir = kagglehub.dataset_download(\"bloox2/fieldplant\")\n",
    "image_dir = Path(image_dir) / \"train\"\n",
    "print(\"Path to dataset files:\", image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62d7c5-a1aa-41ca-94bc-6a77bc736d81",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2498b5-beb6-4719-9187-aad8c0b1ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_preprocessing(app_name):\n",
    "    app = getattr(keras.applications, app_name)\n",
    "    model_name = dir(app)[0]\n",
    "    model = getattr(app, model_name)\n",
    "    input_shape = model().input_shape[1:]\n",
    "    model = model(include_top=False, input_shape=input_shape)\n",
    "    model.trainable = False\n",
    "    preprocessing = getattr(app, \"preprocess_input\")\n",
    "    return model, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4db47e-824e-4444-8229-20c5ab4214d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_classifier_layers(model_name):\n",
    "    if \"vgg\" in model_name:\n",
    "        return keras.Sequential([\n",
    "            keras.layers.GlobalAveragePooling2D(),\n",
    "            keras.layers.Dense(1024, activation=\"relu\"),\n",
    "            keras.layers.Dense(1024, activation=\"relu\") ])\n",
    "    else:\n",
    "        return keras.Sequential([keras.layers.GlobalAveragePooling2D()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41c77b0-169c-482a-85a8-cbf086a33f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(app_name, activation, num_classes):\n",
    "    model, preprocessing = get_model_and_preprocessing(app_name)\n",
    "    pre_classifier_layers = get_pre_classifier_layers(model.name)\n",
    "\n",
    "    inputs = keras.Input(shape=model.input_shape[1:])\n",
    "    x = preprocessing(inputs)\n",
    "    x = model(x, training=False)\n",
    "    x = pre_classifier_layers(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation=activation, name=\"classifier_layer\")(x)\n",
    "\n",
    "    model_name = model.name\n",
    "    model = keras.Model(inputs, outputs, name=model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db5128bc-fe08-46fe-8f41-6c5705e4d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(methodology):\n",
    "    multilabel = methodology == \"multilabel\"\n",
    "    \n",
    "    methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    losses = [\"categorical_crossentropy\", \"binary_crossentropy\"]\n",
    "    activation = [\"softmax\", \"sigmoid\"]\n",
    "    metrics = [\"categorical_accuracy\", \"binary_accuracy\"]\n",
    "\n",
    "    idx = methodologies.index(methodology)\n",
    "\n",
    "    metrics = [metrics[idx]]\n",
    "    f1_score_weighted = keras.metrics.F1Score(average=\"weighted\", threshold=0.5 if multilabel else None, name=\"f1_score_weighted\", dtype=None)\n",
    "    f1_score_per_class = keras.metrics.F1Score(average=None, threshold=0.5 if multilabel else None, name=\"f1_score_per_class\", dtype=None)\n",
    "    metrics.append(f1_score_weighted)\n",
    "    metrics.append(f1_score_per_class)\n",
    "        \n",
    "    hyperparams = [losses[idx], activation[idx], metrics]\n",
    "    \n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52871059-9ca3-435f-9cf0-d738f597d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model):\n",
    "    compile_config = model._compile_config.config\n",
    "    optimizer = compile_config['optimizer'].get_config()\n",
    "    classifier_activation = model.get_layer(name=\"classifier_layer\").activation.__name__\n",
    "\n",
    "    print(\"Model name:\", model.name)\n",
    "    print(\"Input shape:\", model.input_shape)\n",
    "    print(\"Optimizer name:\", optimizer['name'], \"learning_rate:\", np.round(optimizer['learning_rate'], 6))\n",
    "    print(\"Loss:\", compile_config['loss'])\n",
    "    print(\"Metrics:\")\n",
    "    for metric in compile_config['metrics']:\n",
    "        print(metric if isinstance(metric, str) else metric.get_config())\n",
    "    print(\"Classifier layer activation function:\", classifier_activation)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08bb57e7-5137-4267-9bd0-fc7dc629e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name, fine_tuning=False):\n",
    "    cbs = [\n",
    "        keras.callbacks.EarlyStopping(patience=PATIENCE, restore_best_weights=True, baseline=None, verbose=1),\n",
    "        keras.callbacks.ModelCheckpoint(filepath=f\"{saved_models_dir}/{model_name}.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1, initial_value_threshold=None)]\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d421be4-0a10-4ad9-8877-ed16835789b6",
   "metadata": {},
   "source": [
    "# GET DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bcc6399-8c40-434a-b4ff-082e09a8e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(filtered=False, sample=False):\n",
    "    filename = \"filtered\" if filtered else \"unfiltered\"\n",
    "    all_csv_files = list(Path(\"../data\").glob(\"*\"))\n",
    "    csv_file = [csv for csv in all_csv_files if filename in csv.name][0]\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print()\n",
    "\n",
    "    if sample:\n",
    "        print(\"Using sampled DF\")\n",
    "        df = df.sample(frac=0.2)\n",
    "        df = df.loc[(df!=0).any(axis=1)]\n",
    "    print(\"df shape:\", df.shape)\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c10facc-e91a-4771-8658-ed1f7aa96832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_splits(df, filtered=False, test_size=0.2):\n",
    "    col_names = list(df.columns)\n",
    "    split_fn = get_stratified_splits if filtered else get_nonstratified_splits\n",
    "    (X_train, X_test, X_val, y_train, y_test, y_val) = split_fn(df, test_size=test_size)\n",
    "    train_df = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "    test_df = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    val_df = pd.merge(X_val, y_val, left_index=True, right_index=True)\n",
    "    \n",
    "    train_df.columns = col_names\n",
    "    test_df.columns = col_names\n",
    "    val_df.columns = col_names\n",
    "\n",
    "    return train_df, test_df, val_df\n",
    "    \n",
    "def get_stratified_splits(df, test_size=0.2):\n",
    "    columns = list(df.columns)\n",
    "    X = df.filename.to_frame().to_numpy()\n",
    "    y = df.drop(columns=[\"filename\"]).to_numpy()\n",
    "\n",
    "    X_train, y_train, X_test_val, y_test_val = iterative_train_test_split(X, y, test_size=0.2)\n",
    "    X_test, y_test, X_val, y_val = iterative_train_test_split(X_test_val, y_test_val, test_size=0.5)\n",
    "    datasets = (X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "    datasets = [pd.DataFrame(dataset) for dataset in datasets]\n",
    "    return tuple(datasets)\n",
    "\n",
    "def get_nonstratified_splits(df, test_size=0.2):\n",
    "    X = df.filename\n",
    "    y = df.drop(columns=[\"filename\"])\n",
    "    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=SEED)\n",
    "\n",
    "    return (X_train, X_test, X_val, y_train, y_test, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b408146a-c6b0-4f73-94b1-e49a828aa3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(filename, img_size):\n",
    "    filepath = str(image_dir) + \"/\" + filename\n",
    "    img = tf.io.read_file(filepath)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img\n",
    "    \n",
    "def process_dataset(filename, labels, img_size):\n",
    "    img = decode_img(filename, img_size=img_size)\n",
    "    return img, labels\n",
    "\n",
    "def configure_datasets_for_performance(datasets, shuffle=False, batch_size=BATCH_SIZE):\n",
    "    configured_datasets = []\n",
    "    for dataset in datasets:\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=dataset.cardinality(), reshuffle_each_iteration=True)\n",
    "        dataset = dataset.batch(batch_size=batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        configured_datasets.append(dataset)\n",
    "    return tuple(configured_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e76af0-d22b-44d6-975e-625de5e042f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_from_dataframes(img_size, splits=None):\n",
    "    datasets = []\n",
    "    for split in splits:\n",
    "        img = split.filename\n",
    "        labels = split.drop(columns=[\"filename\"])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((img, labels))\n",
    "        dataset = dataset.map(lambda x,y: process_dataset(x,y, img_size))\n",
    "        datasets.append(dataset)\n",
    "    return tuple(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba6b97a-0aab-435c-98c4-362be55d13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(img_size, df, test_size=0.2):\n",
    "    splits = get_train_test_splits(df, test_size=test_size)\n",
    "    datasets = datasets_from_dataframes(img_size, splits=splits)\n",
    "    for dataset in zip([\"train\", \"val\", \"test\"], datasets):\n",
    "        print(dataset[0],\"size:\", len(dataset[1]))\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515feb0-4819-43c6-8a91-06e8ab3f21c8",
   "metadata": {},
   "source": [
    "# CREATE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eea7cf9-d314-4cfa-8f2b-6efd5d4bd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(epochs=5, sample=True):\n",
    "    i = 0\n",
    "    num_models = 0\n",
    "\n",
    "#    app_names = [\"mobilenet_v2\", \"vgg16\", \"inception_v3\", \"inception_resnet_v2\"]\n",
    "    # app_names = [\"mobilenet_v2\", \"vgg16\"]\n",
    "    app_names = [\"vgg16\"]\n",
    "    \n",
    "    methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    filter_options = [False, True]\n",
    "\n",
    "    for filtered in filter_options:\n",
    "        for methodology in methodologies:\n",
    "            if not filtered and (methodology == \"multilabel\"):\n",
    "                continue\n",
    "            for app_name in app_names:\n",
    "                num_models += 1\n",
    "\n",
    "    for filtered in filter_options:\n",
    "        for methodology in methodologies:\n",
    "            if not filtered and (methodology == \"multilabel\"):\n",
    "                continue\n",
    "                \n",
    "            loss, activation, metrics = get_hyperparameters(methodology)\n",
    "    \n",
    "            df = get_dataframe(filtered=filtered, sample=sample)\n",
    "            num_classes = len(df.columns[1:])\n",
    "            \n",
    "            for app_name in app_names:\n",
    "                i += 1\n",
    "                print()\n",
    "                print(f\"Model: {i} of {num_models}\")\n",
    "                print(\"Filtered dataset:\", filtered)\n",
    "                print(\"Methodology:\", methodology)\n",
    "       \n",
    "                model = build_model(app_name, activation=activation, num_classes=num_classes)\n",
    "\n",
    "                img_size = model.input_shape[1:3]\n",
    "                \n",
    "                datasets = get_datasets(img_size, df=df)\n",
    "                train_ds, test_ds, val_ds = configure_datasets_for_performance(datasets)\n",
    "\n",
    "                model.name = model.name + \"_\" + (\"filtered\" if filtered else \"unfiltered\") + \"_\" + methodology\n",
    "                print(\"Save file name:\", model.name)\n",
    "\n",
    "                history_clf = train_classifier(model=model, train_ds=train_ds, val_ds=val_ds, epochs=epochs, loss=loss, metrics=metrics, fine_tuning=False)\n",
    "                history_ft = train_fine_tuning(model=model, train_ds=train_ds, val_ds=val_ds, epochs=epochs, loss=loss, metrics=metrics, fine_tuning=True)\n",
    "                               \n",
    "                history_df.to_csv(\n",
    "                        str(histories_dir) + \"/\" + save_filename + \".csv\",\n",
    "                        index=False)              \n",
    "\n",
    "                keras.utils.clear_session(free_memory=True)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53118a56-c690-4d12-b276-22e2c2f6ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, loss=None, metrics=[], fine_tuning=False):\n",
    "    BASE_LR = 0.01\n",
    "    lr = BASE_LR if not fine_tuning else BASE_LR / 10\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        metrics=metrics\n",
    "    )\n",
    "    print_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef3b3a23-a3bf-49c8-8941-2fc544763b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model=None, train_ds=None, val_ds=None, epochs=100, loss=None, metrics=[], fine_tuning=False):\n",
    "    compile_model(model, loss=loss, metrics=metrics, fine_tuning=fine_tuning)\n",
    "   \n",
    "    cbs = get_callbacks(model, fine_tuning=False)\n",
    "    \n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=cbs)\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df['model'] = model.name\n",
    "    history_df['epoch'] = history.epoch\n",
    "    history_df['type'] = \"CLF\"\n",
    "    return history_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c61b820b-dddf-4031-aef7-0bb288a58e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning(model=None, train_ds=None, val_ds=None, epochs=100, callbacks=[], loss=None, metrics=[], fine_tuning=True):\n",
    "    compile_model(model, loss=loss, metrics=metrics, fine_tuning=fine_tuning)\n",
    "    \n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=cbs)\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df['model'] = model.name\n",
    "    history_df['epoch'] = history.epoch\n",
    "    history_df['type'] = \"FT\"\n",
    "    return history_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f912e95-e514-4310-8ead-fad2a7b817b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df shape: (5156, 28)\n",
      "\n",
      "Model: 1 of 3\n",
      "Filtered dataset: False\n",
      "Methodology: multiclass\n",
      "train size: 4124\n",
      "val size: 516\n",
      "test size: 516\n",
      "Save file name: vgg16_unfiltered_multiclass\n",
      "Model name: vgg16_unfiltered_multiclass\n",
      "Input shape: (None, 224, 224, 3)\n",
      "Optimizer name: adam learning_rate: 0.01\n",
      "Loss: categorical_crossentropy\n",
      "Metrics:\n",
      "categorical_accuracy\n",
      "{'name': 'f1_score_weighted', 'dtype': 'float32', 'average': 'weighted', 'threshold': None}\n",
      "{'name': 'f1_score_per_class', 'dtype': 'float32', 'average': None, 'threshold': None}\n",
      "Classifier layer activation function: softmax\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744065419.290913   70807 service.cc:152] XLA service 0x7fccfc012800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744065419.291089   70807 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-04-07 23:36:59.818069: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1744065420.517431   70807 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-04-07 23:37:01.415350: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_694', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2025-04-07 23:37:01.630506: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_694', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  2/258\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 90ms/step - categorical_accuracy: 0.0156 - f1_score_per_class: 0.0025 - f1_score_weighted: 0.0040 - loss: 5.6981             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744065427.493880   70807 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m257/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - categorical_accuracy: 0.1889 - f1_score_per_class: 0.0143 - f1_score_weighted: 0.0747 - loss: 12.0734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 23:37:24.153136: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_694', 424 bytes spill stores, 424 bytes spill loads\n",
      "\n",
      "2025-04-07 23:37:24.985502: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_694', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - categorical_accuracy: 0.1889 - f1_score_per_class: 0.0143 - f1_score_weighted: 0.0747 - loss: 12.0770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 23:37:31.782953: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_324', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-07 23:37:32.499298: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_331', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-04-07 23:37:32.714200: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_338', 608 bytes spill stores, 608 bytes spill loads\n",
      "\n",
      "2025-04-07 23:37:32.793613: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_338', 412 bytes spill stores, 396 bytes spill loads\n",
      "\n",
      "2025-04-07 23:37:37.058226: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_331', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "2025-04-07 23:37:37.521206: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_338', 668 bytes spill stores, 668 bytes spill loads\n",
      "\n",
      "2025-04-07 23:37:37.565302: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_338', 416 bytes spill stores, 388 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 12.99444, saving model to ../saved_models/<Functional name=vgg16_unfiltered_multiclass, built=True>.keras\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 128ms/step - categorical_accuracy: 0.1890 - f1_score_per_class: 0.0143 - f1_score_weighted: 0.0747 - loss: 12.0805 - val_categorical_accuracy: 0.2209 - val_f1_score_per_class: 0.0141 - val_f1_score_weighted: 0.0856 - val_loss: 12.9944\n",
      "Epoch 2/5\n",
      "\u001b[1m257/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - categorical_accuracy: 0.2184 - f1_score_per_class: 0.0135 - f1_score_weighted: 0.0785 - loss: 13.0905\n",
      "Epoch 2: val_loss did not improve from 12.99444\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - categorical_accuracy: 0.2183 - f1_score_per_class: 0.0135 - f1_score_weighted: 0.0785 - loss: 13.0916 - val_categorical_accuracy: 0.2209 - val_f1_score_per_class: 0.0141 - val_f1_score_weighted: 0.0856 - val_loss: 12.9944\n",
      "Epoch 3/5\n",
      "\u001b[1m257/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - categorical_accuracy: 0.2184 - f1_score_per_class: 0.0135 - f1_score_weighted: 0.0785 - loss: 13.0905\n",
      "Epoch 3: val_loss did not improve from 12.99444\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 60ms/step - categorical_accuracy: 0.2183 - f1_score_per_class: 0.0135 - f1_score_weighted: 0.0785 - loss: 13.0916 - val_categorical_accuracy: 0.2209 - val_f1_score_per_class: 0.0141 - val_f1_score_weighted: 0.0856 - val_loss: 12.9944\n",
      "Epoch 4/5\n",
      "\u001b[1m257/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - categorical_accuracy: 0.2184 - f1_score_per_class: 0.0135 - f1_score_weighted: 0.0785 - loss: 13.0905\n",
      "Epoch 4: val_loss did not improve from 12.99444\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - categorical_accuracy: 0.2183 - f1_score_per_class: 0.0135 - f1_score_weighted: 0.0785 - loss: 13.0916 - val_categorical_accuracy: 0.2209 - val_f1_score_per_class: 0.0141 - val_f1_score_weighted: 0.0856 - val_loss: 12.9944\n",
      "Epoch 5/5\n",
      "\u001b[1m257/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - categorical_accuracy: 0.2184 - f1_score_per_class: 0.0135 - f1_score_weighted: 0.0785 - loss: 13.0905\n",
      "Epoch 5: val_loss did not improve from 12.99444\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 60ms/step - categorical_accuracy: 0.2183 - f1_score_per_class: 0.0135 - f1_score_weighted: 0.0785 - loss: 13.0916 - val_categorical_accuracy: 0.2209 - val_f1_score_per_class: 0.0141 - val_f1_score_weighted: 0.0856 - val_loss: 12.9944\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mrun_all\u001b[39m\u001b[34m(epochs, sample)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSave file name:\u001b[39m\u001b[33m\"\u001b[39m, model.name)\n\u001b[32m     46\u001b[39m history_clf = train_classifier(model=model, train_ds=train_ds, val_ds=val_ds, epochs=epochs, loss=loss, metrics=metrics, fine_tuning=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m history_ft = train_fine_tuning(model=model, train_ds=\u001b[43mtrain\u001b[49m-ds, val_ds=val_ds, epochs=epochs, loss=loss, metrics=metrics, fine_tuning=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     49\u001b[39m history_df.to_csv(\n\u001b[32m     50\u001b[39m         \u001b[38;5;28mstr\u001b[39m(histories_dir) + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + save_filename + \u001b[33m\"\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m         index=\u001b[38;5;28;01mFalse\u001b[39;00m)              \n\u001b[32m     53\u001b[39m keras.utils.clear_session(free_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "model = run_all(epochs=5, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42275b9b-0807-441e-ab6e-9a33b466007e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
