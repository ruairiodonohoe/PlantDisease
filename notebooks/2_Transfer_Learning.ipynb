{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed7a41-d76f-4485-bd2d-4585b67de3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.applications as apps\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199ab32-2dd9-4954-89bc-f79724d31950",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acee17-a6f9-4b36-99c3-ceb2495b8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_memory_growth():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "      except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "def set_memory_limit(memory_limit):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=memory_limit)]\n",
    "        )\n",
    "\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "set_memory_limit(4096)\n",
    "#set_memory_growth()\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd067d-0574-40b9-8056-6f005779c464",
   "metadata": {},
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e5f2a-3699-43cb-8a37-230ab0540cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version of data\n",
    "image_dir = kagglehub.dataset_download(\"bloox2/fieldplant\")\n",
    "image_dir = Path(image_dir)\n",
    "print(\"Path to dataset files:\", image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62d7c5-a1aa-41ca-94bc-6a77bc736d81",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2498b5-beb6-4719-9187-aad8c0b1ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_preprocessing(app_name):\n",
    "    app = getattr(keras.applications, app_name)\n",
    "    model_name = dir(app)[0]\n",
    "    model = getattr(app, model_name)\n",
    "    input_shape = model().input_shape[1:]\n",
    "    model = model(include_top=False, input_shape=input_shape)\n",
    "    model.trainable = False\n",
    "    preprocessing = getattr(app, \"preprocess_input\")\n",
    "    return model, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c77b0-169c-482a-85a8-cbf086a33f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(app_name, activation, num_classes):\n",
    "    model, preprocessing = get_model_and_preprocessing(app_name)\n",
    "\n",
    "    inputs = keras.Input(shape=model.input_shape[1:])\n",
    "    x = preprocessing(inputs)\n",
    "    x = model(x, training=False)\n",
    "    \n",
    "    outputs = keras.layers.Dense(num_classes, activation=activation, name=\"classifier_layer\")(x)\n",
    "\n",
    "    model_name = model.name\n",
    "    model = keras.Model(inputs, outputs, name=model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5128bc-fe08-46fe-8f41-6c5705e4d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(methodology):\n",
    "    multilabel = methodology == \"multilabel\"\n",
    "    \n",
    "    methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    losses = [\"categorical_crossentropy\", \"binary_crossentropy\"]\n",
    "    activation = [\"softmax\", \"sigmoid\"]\n",
    "    metrics = [\"categorical_accuracy\", \"binary_crossentropy\"]\n",
    "\n",
    "    idx = methodologies.index(methodology)\n",
    "\n",
    "    metrics = [metrics[idx]]\n",
    "    f1_score_weighted = keras.metrics.F1Score(average=\"weighted\", threshold=0.5 if multilabel else None, name=\"f1_score_weighted\", dtype=None)\n",
    "    f1_score_per_class = keras.metrics.F1Score(average=None, threshold=0.5 if multilabel else None, name=\"f1_score_per_class\", dtype=None)\n",
    "    metrics.append(f1_score_weighted)\n",
    "    metrics.append(f1_score_per_class)\n",
    "        \n",
    "    hyperparams = [losses[idx], activation[idx], metrics]\n",
    "    \n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52871059-9ca3-435f-9cf0-d738f597d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model):\n",
    "    compile_config = model._compile_config.config\n",
    "    optimizer = compile_config['optimizer'].get_config()\n",
    "    classifier_activation = model.get_layer(name=\"classifier_layer\").activation.__name__\n",
    "\n",
    "    print(\"Model name:\", model.name)\n",
    "    print(\"Input shape:\", model.input_shape)\n",
    "    print(\"Optimizer name:\", optimizer['name'], \"learning_rate:\", np.round(optimizer['learning_rate'], 6))\n",
    "    print(\"Loss:\", compile_config['loss'])\n",
    "    print(\"Metrics:\", compile_config['metrics'])\n",
    "    print(\"Classifier layer activation function:\", classifier_activation)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb57e7-5137-4267-9bd0-fc7dc629e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    cbs = [\n",
    "        callbacks.EarlyStopping(patience=PATIENCE, restore_best_weights=True, baseline=None, verbose=1),\n",
    "        callbacks.ModelCheckpoint(filepath=f\"{saved_models_dir}{model.name}.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1, initial_value_threshold=None)]\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d421be4-0a10-4ad9-8877-ed16835789b6",
   "metadata": {},
   "source": [
    "# GET DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc6399-8c40-434a-b4ff-082e09a8e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(filtered=False):\n",
    "    filename = \"filtered\" if filtered else \"unfiltered\"\n",
    "    all_csv_files = list(Path(\"../data\").glob(\"*\"))\n",
    "    csv_file = [csv for csv in all_csv_files if filename in csv.name][0]\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10facc-e91a-4771-8658-ed1f7aa96832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_splits(df, filtered=False, test_size=0.2):\n",
    "    col_names = list(df.columns)\n",
    "    split_fn = get_stratified_splits if filtered else get_nonstratified_splits\n",
    "    (X_train, X_test, X_val, y_train, y_test, y_val) = split_fn(df, test_size=test_size)\n",
    "    train_df = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "    test_df = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "    val_df = pd.merge(X_val, y_val, left_index=True, right_index=True)\n",
    "    \n",
    "    train_df.columns = col_names\n",
    "    test_df.columns = col_names\n",
    "    val_df.columns = col_names\n",
    "\n",
    "    return train_df, test_df, val_df\n",
    "    \n",
    "def get_stratified_splits(df, test_size=0.2):\n",
    "    columns = list(df.columns)\n",
    "    X = df.filename.to_frame().to_numpy()\n",
    "    y = df.drop(columns=[\"filename\"]).to_numpy()\n",
    "\n",
    "    X_train, y_train, X_test_val, y_test_val = iterative_train_test_split(X, y, test_size=0.2)\n",
    "    X_test, y_test, X_val, y_val = iterative_train_test_split(X_test_val, y_test_val, test_size=0.5)\n",
    "    datasets = (X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "    datasets = [pd.DataFrame(dataset) for dataset in datasets]\n",
    "    return tuple(datasets)\n",
    "\n",
    "def get_nonstratified_splits(df, test_size=0.2):\n",
    "    X = df.filename\n",
    "    y = df.drop(columns=[\"filename\"])\n",
    "    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=SEED)\n",
    "\n",
    "    return (X_train, X_test, X_val, y_train, y_test, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408146a-c6b0-4f73-94b1-e49a828aa3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(filename, img_size):\n",
    "    filepath = str(image_dir) + filename\n",
    "    img = tf.io.read_file(filepath)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img\n",
    "    \n",
    "def process_dataset(filename, labels, img_size):\n",
    "    img = decode_img(filename, img_size=img_size)\n",
    "    return img, labels\n",
    "\n",
    "def configure_dataset_for_performance(dataset, shuffle=False, batch_size=BATCH_SIZE):\n",
    "    dataset = dataset.cache()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=dataset.cardinality(), reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(batch_size=batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e76af0-d22b-44d6-975e-625de5e042f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_from_dataframes(img_size, splits=None):\n",
    "    datasets = []\n",
    "    for split in splits:\n",
    "        img = split.filename\n",
    "        labels = split.drop(columns=[\"filename\"])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((img, labels))\n",
    "        dataset = dataset.map(lambda x,y: process_dataset(x,y, img_size))\n",
    "        dataset = configure_dataset_for_performance(dataset)\n",
    "        datasets.append(dataset)\n",
    "    return tuple(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6b97a-0aab-435c-98c4-362be55d13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(img_size, filtered=False, test_size=0.2):\n",
    "    df = get_dataframe(filtered)\n",
    "    splits = get_train_test_splits(df, filtered=filtered, test_size=test_size)\n",
    "    datasets = datasets_from_dataframes(img_size, splits=splits)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515feb0-4819-43c6-8a91-06e8ab3f21c8",
   "metadata": {},
   "source": [
    "# CREATE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea7cf9-d314-4cfa-8f2b-6efd5d4bd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(epochs=5):\n",
    "    i = 0\n",
    "    app_names = [\"mobilenet_v2\", \"vgg16\", \"inception_v3\", \"inception_resnet_v2\"]\n",
    "    #app_names = [\"mobilenet_v2\"]\n",
    "    methodologies = [\"multiclass\", \"multilabel\"]\n",
    "    methodologies = [\"multiclass\"]\n",
    "\n",
    "\n",
    "    for filtered in [False, True]:\n",
    "        for methodology in methodologies:\n",
    "            if not filtered and (methodology == \"binary\"):\n",
    "                continue\n",
    "                \n",
    "            loss, activation, metrics = get_hyperparameters(methodology)\n",
    "    \n",
    "            for filtered in [False, True]:\n",
    "                df = get_dataframe(filtered=filtered)\n",
    "                num_classes = len(df.columns[1:])\n",
    "                \n",
    "                for app_name in app_names:\n",
    "                    i += 1\n",
    "                    print(\"Model:\", i)\n",
    "                    print(\"Filtered dataset:\", filtered)\n",
    "                    print(\"Methodology:\", methodology)\n",
    "           \n",
    "                    model = build_model(app_name, activation=activation, num_classes=num_classes)\n",
    "                    img_size = model.input_shape[1:3]\n",
    "                    ds = get_datasets(img_size, filtered=filtered)\n",
    "                \n",
    "                    # model.compile(\n",
    "                    #     loss=loss,\n",
    "                    #     optimizer=keras.optimizers.Adam(learning_rate=BASE_LR),\n",
    "                    #     metrics=metrics\n",
    "                    # )\n",
    "                    # print_model_info(model)\n",
    "        \n",
    "                    # cbs = get_callbacks()\n",
    "        \n",
    "                    # history = model.fit(train_ds, validation_data=val_ds,\n",
    "                    #                     epochs=epochs, callbacks=cbs)\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f912e95-e514-4310-8ead-fad2a7b817b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d4eca-1bef-4873-9115-0d0bc41f9be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914aa31-500a-4966-a4d4-2677602d0a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c3d97-dc08-4f6c-9eb8-b42d608bc4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0786f-9daf-42b4-ac9e-87904fb47a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c69e55-cf66-4545-a663-857dc99733c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
